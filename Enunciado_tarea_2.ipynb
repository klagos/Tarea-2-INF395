{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Enunciado_tarea_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_LwBzSZFjl2"
      },
      "source": [
        "<hr style=\"height:2px;border:none\"/>\n",
        "<h1 align='center'> <img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
        "\n",
        "INF-395 / 477 / 577 Tarea 2 Redes Neuronales Artificiales - 2020-2 </h1>\n",
        "\n",
        "<H3 align='center'> Integrantes: ...... - ....... </H3>\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "\n",
        "**Temas**  \n",
        "* Manipulaciones en tensorflow, keras, pandas y numpy\n",
        "* Recurrent Neural Networks\n",
        "* LSTM, GRU\n",
        "* Autoencoders\n",
        "* GAN\n",
        "\n",
        "**Formalidades**  \n",
        "* Equipos de trabajo de 2 personas (*Ambos estudiantes deben estar preparados para presentar la tarea el día de la entrega*)\n",
        "* El entregable debe ser un _Jupyter Notebook_ incluyendo los códigos utilizados, los resultados, los gráficos realizados y comentarios. Debe seguir una estructura similar a un informe (se debe introducir los problemas a trabajar, presentar los resultados y discutirlos). Si lo prefiere puede entregar un _Jupyter Notebook_ por pregunta o uno para toda la tarea, con tal de que todos los entregables estén bien identificados y se encuentren en el mismo repositorio de _Github_.\n",
        "* Se debe preparar una presentación del trabajo realizado y sus hallazgos. El presentador será elegido aleatoriamente y deberá apoyarse en el _Jupyter Notebook_ que entregarán. \n",
        "* Formato de entrega: envı́o de link del repositorio en _Github_ ( en caso de ser repositorio privado, invitar como colaborador al usuario de github \"Aerlio\") al correo electrónico del ayudante (*<tomas.ochoa.14@sansano.usm.cl>*), en copia al profesor (*<cvalle@inf.utfsm.cl>*). Especificar el siguiente asunto: [INF395/477/577-2020 Tarea 2]\n",
        "* Fecha de entrega y presentaciones: 8 de Enero. Hora límite de entrega: 23:00. Cualquier _commit_ luego de la hora límite no será evaluado. Se realizará descuento por atrasos en envío del mail. \n",
        "\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "\n",
        "La tarea se divide en tres partes:\n",
        "\n",
        "[1.](#primero) RNNs para series de tiempo  <br>\n",
        "[2.](#segundo) RNNs para texto <br>\n",
        "[3.](#tercero) Autoencoders para imágenes <br>\n",
        "[3.](#cuarto) GANs para imágenes <br>\n",
        "\n",
        "La tarea tiene ejemplos de códigos con los cuales pueden guiarse en gran parte, sin embargo solo son guías y pueden ser creativos al momento de resolver la tarea. Soluciones creativas o elegantes serán valoradas. También en algunas ocaciones se hacen elecciones arbitrarias, ustedes pueden realizar otras elecciones con tal de que haya una pequeña justificación de por qué su elección es mejor o equivalente.\n",
        "Recuerden intercalar su código con comentarios y con celdas _Markdown_ con los comentarios de la pregunta y con cualquier analisis, fórmula o explicación que les parezca relevante para justificar sus procedimientos. \n",
        "Noten que en general cuando se les pide elegir algo o proponer algo no se evaluará mucho la elección en si, en cambio la argumentación detrás de la elección será lo más ponderado.\n",
        "\n",
        "**Es ÁLTAMENTE recomendado realizar esta tarea en _Colab_ de Google (https://colab.research.google.com/notebooks/intro.ipynb#recent=true), con el fin de no depender del rendimiento de su computador personal al momento de entrenar redes neuronales y poder compartir de forma fácil sus avances con su compañer@ de trabajo.** Si bien conlleva sus pros y contras utilizar _Colab_ , existirá una curva de aprendizaje personal que lo ayudará a sacar el mayor provecho a esta herramienta, por ejemplo aprendiendo a guardar los avances realizados, evitando tener que ejecutar todo el código cada vez que se abra _Colab_ . *Tip: Una vez abierto un notebook en _Colab_ ir a **entorno de ejecución**->**Cambiar tipo de entorno de ejecución**, y seleccionár TPU como acelerador por hardware para redes recurrentes y GPU para redes convolucionales.*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwJAApzbF4Of"
      },
      "source": [
        "# 1. RNNs para series de tiempo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gMDgpLG_TkD"
      },
      "source": [
        "Las redes neuronales recurrentes (RNNs) son una red neuronal profunda que tiene, como su nombre indica, entradas recurrentes en la capa oculta, es decir, la salida de una capa oculta se retroalimenta a sí misma. La memoria neuronal es la capacidad impartida a un modelo para retener la entrada de los pasos de tiempo anteriores cuando la entrada es secuencial. En términos simples, cuando nuestro problema está asociado con una secuencia de datos como una oración o una serie temporal o la letra de una canción, el modelo tiene que recordar los estados previos de la entrada para funcionar.\n",
        "\n",
        "<h1 align='center'> <img src=\"https://www.hobodataloggers.com.au/images/thumbs/0007371_hobo-weather-station-kits_510.jpeg\" width=\"50%\" height=\"100%\" /> </h1>\n",
        "\n",
        "En esta pregunta trabajaremos con datos obtenidos presumiblemente de una estación meteorológica, los cuales se obtienen desde el sitio https://www.kaggle.com/dronio/SolarEnergy, este dataset contiene mediciones de los últimos 4 meses. Nuestro objetivo será predecir el nivel de radiación solar para la próximas 24 horas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TadtgytHIXFp"
      },
      "source": [
        "## 1.a Carga de datos y preprocesamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nujeesYmts-a"
      },
      "source": [
        "##### I) Iniciaremos cargando los datos. Para esto necesitará crear una cuenta en kaggle, dirigirse a su perfil, ir a Account, y en la sección API apretar _Create new API token_ , se descargará un archivo kaggle.json, ábralo como archivo de texto y obtenga su username y key. Luego ejecute el siguiente código (desconozco por qué pero a veces hay que ejecutar el código 2 veces para que funcione). Solución obtenida desde el hilo: https://gist.github.com/jayspeidell/d10b84b8d3da52df723beacc5b15cb27"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRBEyJOar8M_"
      },
      "source": [
        "username=\"\"\n",
        "key=\"\"\n",
        "!pip install -q kaggle\n",
        "api_token = {\"username\":username,\"key\":key}\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = str(username)\n",
        "os.environ['KAGGLE_KEY'] = str(key)\n",
        "!kaggle datasets download -d dronio/SolarEnergy\n",
        "if not os.path.exists(\"/content/solar_prediction\"):\n",
        "    os.makedirs(\"/content/solar_prediction\")\n",
        "os.chdir('/content/solar_prediction')\n",
        "for file in os.listdir():\n",
        "    if file[-4:]==\".zip\":\n",
        "      zip_ref = zipfile.ZipFile(file, 'r')\n",
        "      zip_ref.extractall()\n",
        "      zip_ref.close()\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CULz-BF_FB5Q"
      },
      "source": [
        "Procederemos a ordenar la serie temporalmente a continuación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x09Tz0-WEwos"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "import pytz\n",
        "hawaii= timezone('Pacific/Honolulu') # to convert unixtime\n",
        "dataset = pd.read_csv('SolarPrediction.csv') # read data\n",
        "dataset = dataset.sort_values(['UNIXTime'], ascending = [True]) #sort data\n",
        "dataset.index =  pd.to_datetime(dataset['UNIXTime'], unit='s') # make unix pandas dataframe index\n",
        "dataset.index = dataset.index.tz_localize(pytz.utc).tz_convert(hawaii) # convert unixtime to timestamp\n",
        "dataset.drop(['Data','Time','TimeSunRise','TimeSunSet',\"UNIXTime\"], inplace=True, axis=1) # drop extra-time variables\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibeKMwIbIJDW"
      },
      "source": [
        "**Comentario:** Se puede observar inmediatamente que existen datos faltantes, los cuales no son declarados. Dado que de los 6 datos impresos se puede inferir que el timestep es de 5 minutos, y faltarían los datos del instante 2016-09-01 ~00:10:00 y 2016-09-01 ~00:15:00"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynK6758JG14A"
      },
      "source": [
        "A continuación se proveen comandos con los cuales se puede observar cómo a partir del timestamp generado se puede categorizar con comandos simples el dataset según minuto del día, hora del día, mes del año, y día del año. Los cuales se ocuparán durante la tarea. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3-ZbTBtGtHl"
      },
      "source": [
        "dataset.index.minute,dataset.index.hour,dataset.index.month,dataset.index.dayofyear"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvgVnZsPH4C7"
      },
      "source": [
        "##### II) Para poder idententificar los Missing Values, y poder contar de manera segura con días de 24 horas, ejecute el siguiente código. **Explique** la funcionalidad del código escrito, escribiendo en forma de comentario (#) sobre las líneas de este código."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsPpNHe9JXGk"
      },
      "source": [
        "import time\n",
        "t1=time.time() \n",
        "new_data=np.nan*np.zeros((24*12*(len(np.unique(dataset.index.dayofyear))),1+len(dataset.columns)), dtype=object)\n",
        "days=np.unique(np.asarray([str(dataset.index[i]).split(\" \")[0] for i in range(dataset.shape[0])]))\n",
        "\n",
        "def min_to_str(min):\n",
        "  if min==0: min=\"00\"\n",
        "  elif min==5: min=\"0\"+str(min)\n",
        "  else: min=str(min)\n",
        "  return(min)\n",
        "\n",
        "for i in range(new_data.shape[0]):\n",
        "  hr=i//12-24*((i//12)//24)\n",
        "  if hr==0: hr=\"00\"\n",
        "  elif 0<hr<10: hr=\"0\"+str(hr)\n",
        "  else: hr=str(hr)\n",
        "  min=5*i-60*((5*i)//60)\n",
        "  day=(i//12)//24\n",
        "  new_data[i,0]=days[day]+\" \"+str(hr)+\":\"+min_to_str(min)+\":00\"\n",
        "\n",
        "\n",
        "verbose,freq=True,12*24*5+12+1 # to see output\n",
        "w_in=0\n",
        "\n",
        "for i in range(dataset.shape[0]):\n",
        "  ind=dataset.index[i]\n",
        "  min=ind.minute\n",
        "  if i>0: past_min=dataset.index[i-1].minute\n",
        "  if i<dataset.shape[0]-1: next_min=dataset.index[i+1].minute\n",
        "  if min%5!=0:\n",
        "    if (min+1)%5==0: min=min+1\n",
        "    elif (min-1)%5==0: min=min-1\n",
        "    elif past_min%5==0:\n",
        "      if past_min!=55: min=past_min+5\n",
        "      else: min=0\n",
        "    elif next_min%5==0:\n",
        "      if next_min!=0: min=past_min-5\n",
        "      else: min=55\n",
        "    elif (past_min-1)%5==0:\n",
        "      if (past_min-1)!=55: min=past_min-1+5\n",
        "      else: min=0\n",
        "    else: print(past_min,min,next_min,\"---keep expanding\")\n",
        "  ind=str(ind)[:-11]+min_to_str(min)+str(ind)[-9:]\n",
        "  found=0\n",
        "  for w in range(w_in,w_in+new_data.shape[0]):\n",
        "    if new_data[w,0][:-2] in ind:\n",
        "      w_in=w\n",
        "      found=1\n",
        "      new_data[w,1:]=dataset.values[i,:]\n",
        "      if verbose and i%freq==0: print(str(dataset.index[i]),\"---->\",new_data[i+w,0])\n",
        "      break\n",
        "  if found==0:\n",
        "    print(\"Error at \",ind)\n",
        "    break\n",
        "\n",
        "df=pd.DataFrame(new_data[:,1:].astype(\"float32\"),index=new_data[:,0],columns=dataset.columns)\n",
        "t2=time.time()\n",
        "print(\"Time of computation (seconds): \", t2-t1)\n",
        "print(\"N° Missing time steps data: \", np.isnan(new_data[:,1].astype(\"float\")).sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0IWrMTSvpEv"
      },
      "source": [
        "##### III) **Reemplace** TODOS los missing values (NaN) de cada columna del nuevo dataset (df) de la forma que estime conveniente. Para esto debe usar los valores respectivos de cada columna, ya sea en el pasado o futuro. <br><br> _Como ejemplo_: Si es que existen datos perdidos a las 23:15 hrs, puede reemplazar el valor NaN con el promedio entre las 23:10 y 23:20, teniendo cuidado que el valor en la columna a reemplazar p.ej Pressure sea efectivamente el promedio entre la variable Pressure a las 23:10 y 23:20 . Note que el promedio entre un valor númerico y NaN es igual a NaN, por lo que tendrá que encontrar cómo hacer frente a esta situación.  <br><br> **Comente y fundamente su elección.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8dOthgtwoJK"
      },
      "source": [
        "time=pd.to_datetime(df.index)\n",
        "var_names=df.columns.tolist()\n",
        "data=df.copy().values\n",
        "nan_row_loc=np.where((np.isnan((data[:,0]))==True))\n",
        "np.isnan((data)).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjbgXvHERx9k"
      },
      "source": [
        "**Para disminuir los tiempos de entrenamiento reduciremos la resolución temporal de 5-min a 1-hr. Para esto calcularemos el promedio de cada hora.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amme3Vv2R1IY"
      },
      "source": [
        "data_hr=np.nan*np.zeros((int(data.shape[0]/12),data.shape[1]))\n",
        "time_hr=np.nan*np.zeros((int(data.shape[0]/12)), dtype=object)\n",
        "for i in range(data_hr.shape[0]):\n",
        "  data_hr[i,:]=np.mean(data[12*i:12*(i+1),:],axis=0)\n",
        "  time_hr[i]=time[12*i]\n",
        "time_hr=pd.to_datetime(time_hr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc1nF96dHMrE"
      },
      "source": [
        "##### IV) **Separe** el dataset en conjuntos de entrenamiento, validación y test, para esto considere un 70% de los datos para entrenamiento, un 15% para validación y un 15% para test. Como se está trabajando con series de tiempo, proceda a crear los conjuntos de manera sucesiva. Guarde las fechas asociadas a cada dato puesto se utilizarán más adelante. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT_5ck9DHQpa"
      },
      "source": [
        "data_tr, time_tr = data_hr[:int(data_hr.shape[0]*0.7)],time_hr[:int(data_hr.shape[0]*0.7)]\n",
        "data_val, time_val = data_hr[int(data_hr.shape[0]*0.7):...],time_hr[int(data_hr.shape[0]*0.7):...]\n",
        "data_tst, time_tst = data_hr[...:],time_hr[...:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaAufyUjuR0B"
      },
      "source": [
        "##### V) **Comente y visualice** los valores promedios y desviación estándar de cada atributo del dataset de entrenamiento según la hora del día y mes del año. Apóyese en el siguiente código si lo desea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMYKohsM0VgP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "b8f0b09e-ead8-41ee-e714-21622684c44b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "cols=sns.color_palette(\"cubehelix\", len(var_names))\n",
        "J=0\n",
        "var_name=var_names[J]\n",
        "mean_per_hr=np.nan*np.zeros((24))\n",
        "std_per_hr=np.nan*np.zeros((24))\n",
        "for hr in range(23):\n",
        "  mean_per_hr[hr]=np.mean(data_tr[time_tr.hour==hr][:,J])\n",
        "  std_per_hr[hr]=np.std(data_tr[time_tr.hour==hr][:,J])\n",
        "plt.rcParams[\"figure.figsize\"]=[20,2]\n",
        "plt.suptitle(var_name,size=18,y=1.2)\n",
        "plt.subplot(1,2,1),plt.title(\"Mean by hour\",size=16)\n",
        "plt.plot(mean_per_hr,color=cols[J]),plt.grid()\n",
        "plt.subplot(1,2,2),plt.title(\"Std. deviation by hour\",size=16)\n",
        "plt.plot(std_per_hr,color=cols[J]),plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x144 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IOqnBnLGz9A"
      },
      "source": [
        "##### VI) **Estandarice** los conjuntos de entrenamiento, validación y test. Tienendo en mente que buscaremos predecir la radiación para las 24 horas futuras definiremos las variables de entrada y salida, realizando una transformación conveniente utilizando la función split_sequence entregada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkS2ySMKLi1z"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0v1K6-GMq_c"
      },
      "source": [
        "def split_sequence(sequence, n_steps_in, n_steps_out,time_seq):\n",
        "    X, y , seq_t= list(), list(), list()\n",
        "    for i in range(len(sequence)):\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_steps_in\n",
        "        out_end_ix = end_ix + n_steps_out\n",
        "        # check if we are beyond the sequence\n",
        "        if out_end_ix > len(sequence):\n",
        "            break\n",
        "        # gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix,0]\n",
        "        seq_t.append(time_seq[end_ix])\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y) , np.asarray(seq_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ORs5hYM3n2"
      },
      "source": [
        "len_inp=24\n",
        "len_out=24\n",
        "x_tr,y_tr,t_tr=split_sequence(xy_tr_sc,len_inp,len_out,time_tr)\n",
        "x_val,y_val,t_val=split_sequence(xy_val_sc,len_inp,len_out,time_tr)\n",
        "x_tst,y_tst,t_tst=split_sequence(xy_tst_sc,len_inp,len_out,time_tr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRtaViMsXWpa"
      },
      "source": [
        "**Explique** a qué corresponde cada dimensión de las variables de entrada y salida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8BxEJebW4O0"
      },
      "source": [
        "x_tr.shape,x_val.shape,x_tst.shape,y_tr.shape,y_val.shape,y_tst.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSItDyEkOZDK"
      },
      "source": [
        "## 1.b) Primera  red recurrente.\n",
        "Ahora entrenaremos una primera red recurrente LSTM. **Explique** la particularidad de estas redes y **por qué** podría comportarse bien para este tipo de problemas. <br> <br> Entrenaremos dos redes recurrentes similares, la primera tan solo utilizará la variable Radiation como entrada para predecir Radiation, caso que llamaremos **univariante**, mientras la segunda utilizará las variables [Radiation, Temperature, Pressure, Humidity, WindDirection(Degrees), Speed] como entrada para predecir Radiation, caso que llamaremos **multivariante**. <br> <br> **Recupere** las funciones utilizadas en la tarea anterior para graficar los errores de entrenamiento, validación y test, y así comparar el rendimiento de ambas redes RNN gráficamente. **Comente**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqe0iFhkOiQ6"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import activations\n",
        "from tensorflow.python.keras.engine import input_layer\n",
        "\n",
        "input_lstm= input_layer.Input(shape=(x_tr.shape[1],1))\n",
        "rnn=layers.LSTM(units=56,return_sequences=False)(input_lstm)\n",
        "dens=layers.Dense(len_out, activation='sigmoid')(rnn)\n",
        "model_1=models.Model(inputs=input_lstm, outputs=dens)\n",
        "model_1.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "model_1.summary()\n",
        "history_1 = model_1.fit(x_tr[:,:,0:1],y_tr, validation_data=(x_val[:,:,0:1],y_val), epochs=10, batch_size=16,verbose=False)\n",
        "\n",
        "input_lstm= input_layer.Input(shape=(x_tr.shape[1:]))\n",
        "rnn=layers.LSTM(units=56,return_sequences=False)(input_lstm)\n",
        "dens=layers.Dense(len_out, activation='sigmoid')(rnn)\n",
        "model_2=models.Model(inputs=input_lstm, outputs=dens)\n",
        "model_2.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "model_2.summary()\n",
        "history_2 = model_2.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=10, batch_size=16,verbose=False)\n",
        "\n",
        "y_pred_1=model_1(x_tst[:,:,0:1])\n",
        "tst_loss_1=tf.math.reduce_mean(tf.keras.losses.MSE(y_tst, y_pred_1)).numpy()\n",
        "y_pred_2=model_2(x_tst)\n",
        "tst_loss_2=tf.math.reduce_mean(tf.keras.losses.MSE(y_tst, y_pred_2)).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT3visPfheBi"
      },
      "source": [
        "## 1.c) Exploración de profundidad y GRU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxo5rLZwY_70"
      },
      "source": [
        "##### I) Exploraremos aumentar la profundidad de la red neuronal. El modelo LSTM original se compone de una sola capa LSTM oculta seguida de una capa de salida estándar. Podríamos decir que nuestra red neuronal posee dos profundidades distintas. La Stacked LSTM es una extensión del modelo LSTM original, la cual tiene múltiples capas LSTM ocultas donde cada capa contiene múltiples celdas de memoria. A la vez, del mismo modo que en la Tarea 1, es posible aumentar la profundidad de la red densa de salida. \n",
        "\n",
        "Proceda a probar distintas combinaciones de profundidades de capas LSTM y de capas Densas (contando la capa de salida), para el caso univariado y multivariado. Para mantener un tiempo de ejecución produnte, considere una profunidad LSTM máxima de 2, y una profundidad densa máxima de 2. **Grafique y compare** los errores de entrenamiento, validación y test para los distintos casos.  **Comente**.\n",
        "\n",
        "El siguiente código muestra el caso multivariado con profundidades máximas LSTM y Densas. Utilice el número de celdas de memoria y número de neuronas recomendado en el código para las distintas profundidades, al igual que el número de epochs.\n",
        "\n",
        "**Pregunta:** ¿Por qué cuando una capa LSTM precede otra capa LSTM es necesario utilizar return_sequences=True? ¿Es necesario/recomendado utilizar return_sequences=False cuando una capa LSTM precede a una capa Densa?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePwJuiQUdEzP"
      },
      "source": [
        "input_lstm= input_layer.Input(shape=(x_tr.shape[1:]))\n",
        "rnn=layers.LSTM(units=56,return_sequences=True)(input_lstm) # 1° lstm\n",
        "rnn=layers.LSTM(units=56,return_sequences=False)(rnn) # 2° lstm\n",
        "dens=layers.Dense(32, activation='relu')(rnn) # 1° densa\n",
        "dens=layers.Dense(len_out, activation='sigmoid')(dens) # 2° densa\n",
        "model=models.Model(inputs=input_lstm, outputs=dens)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "model.summary()\n",
        "history = model.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=10, batch_size=16,verbose=True)\n",
        "y_pred=model(x_tst)\n",
        "tst_loss=tf.math.reduce_mean(tf.keras.losses.MSE(y_tst, y_pred)).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEN5mu_bgJx0"
      },
      "source": [
        "##### II) Añadiremos entradas a la capa Densa que indiquen la hora del día de forma bi-dimensional, semejante a un reloj, y una variable dummy que indica si es de día o noche. Ejecute el siguiente código para generar las entradas adicionales, y **añada la entrada** a la arquitectura entregada. Esta forma de ingresar variables adicionales a la capa densa se asemeja a la arquitectura skip-connections vista en la Tarea 1. **La hora que se entrega para indicar si es de día o noche es intencionalmente erronea**. Utilizando los datos de radiación del conjunto de entrenamiento **deberá definir las horas de día y noche de manera justificada**. \n",
        "\n",
        "**Comente** sobre los resultados obtenidos.\n",
        "\n",
        "**Pregunta:** ¿Por qué puede resultar beneficioso representar la hora del día de manera bidimensional en lugar de unidimensional? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9fmUh2AnDV4"
      },
      "source": [
        "daytimehrs=[2,22]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ9FNdtYjGuk"
      },
      "source": [
        "from math import *\n",
        "hour_1_tr=np.asarray(np.sin(2*pi*pd.to_datetime(t_tr).hour/24)).reshape(-1,1)\n",
        "hour_2_tr=np.asarray(np.cos(2*pi*pd.to_datetime(t_tr).hour/24)).reshape(-1,1)\n",
        "dummy_tr=(1*(pd.to_datetime(t_tr).hour<daytimehrs[1])*(pd.to_datetime(t_tr).hour>daytimehrs[0])).reshape(-1,1)\n",
        "hour_1_val=np.asarray(np.sin(2*pi*pd.to_datetime(t_val).hour/24)).reshape(-1,1)\n",
        "hour_2_val=np.asarray(np.cos(2*pi*pd.to_datetime(t_val).hour/24)).reshape(-1,1)\n",
        "dummy_val=(1*(pd.to_datetime(t_val).hour<daytimehrs[1])*(pd.to_datetime(t_val).hour>daytimehrs[0])).reshape(-1,1)\n",
        "hour_1_tst=np.asarray(np.sin(2*pi*pd.to_datetime(t_tst).hour/24)).reshape(-1,1)\n",
        "hour_2_tst=np.asarray(np.cos(2*pi*pd.to_datetime(t_tst).hour/24)).reshape(-1,1)\n",
        "dummy_tst=(1*(pd.to_datetime(t_tst).hour<daytimehrs[1])*(pd.to_datetime(t_tst).hour>daytimehrs[0])).reshape(-1,1)\n",
        "\n",
        "ext_tr=np.concatenate((hour_1_tr,hour_2_tr,dummy_tr),axis=1)\n",
        "ext_val=np.concatenate((hour_1_val,hour_2_val,dummy_val),axis=1)\n",
        "ext_tst=np.concatenate((hour_1_tst,hour_2_tst,dummy_tst),axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoDGtxGZexXO"
      },
      "source": [
        "from keras.layers import concatenate\n",
        "\n",
        "input_lstm= input_layer.Input(shape=(x_tr.shape[1:]))\n",
        "rnn=layers.LSTM(units=56,return_sequences=True)(input_lstm) # 1° lstm\n",
        "rnn=layers.LSTM(units=56,return_sequences=False)(rnn) # 2° lstm\n",
        "input_clock= input_layer.Input(shape=(3))\n",
        "input_dense=concatenate([rnn, input_clock])\n",
        "dens=layers.Dense(32, activation='relu')(input_dense) # 1° densa\n",
        "dens=layers.Dense(len_out, activation='sigmoid')(dens) # 2° densa\n",
        "model=models.Model(inputs=[input_lstm,input_clock], outputs=dens)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "model.summary()\n",
        "history = model.fit([x_tr,ext_tr],y_tr, validation_data=([x_val,ext_val],y_val), epochs=10, batch_size=16,verbose=True)\n",
        "y_pred=model([x_tst,ext_tst])\n",
        "tst_loss=tf.math.reduce_mean(tf.keras.losses.MSE(y_tst, y_pred)).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-W1B3sQoSR9"
      },
      "source": [
        "##### III) Entrene la misma arquitectura que en la pregunta anterior utilizando unidades GRU en lugar de LSTM. **Explique la diferencia entre GRU y LSTM**. **Compare** los resultados con los obtenidos en el punto anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMj1c6gZlbjJ"
      },
      "source": [
        "input_lstm= input_layer.Input(shape=(x_tr.shape[1:]))\n",
        "rnn=layers.GRU(units=56,return_sequences=True)(input_lstm) # 1° capa gru"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-b_EsDp_jxA"
      },
      "source": [
        "##### IV) Con la red con mejor rendimiento, respecto al error de validación, en este punto 1.c) ejecute el siguiente código y saque conclusiones del violin plot.\n",
        "\n",
        "**Pregunta:** ¿Qué hace un violin plot?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SXG6WzaBOpU"
      },
      "source": [
        "y_pred=model(x_tst)\n",
        "tst_loss=((y_tst- y_pred)**2).numpy()\n",
        "plt.rcParams[\"figure.figsize\"]=[20,3]\n",
        "plt.title(\".... choose a name \",size=22)\n",
        "plt.violinplot(tst_loss)\n",
        "plt.xlabel(\"Hours ahead\",size=18)\n",
        "plt.ylabel(\"$(y_{true}-y_{pred})^2$\",size=18)\n",
        "plt.xticks(np.arange(25))\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNaqKOfVpS5Y"
      },
      "source": [
        "## 1.d) Parameters & hyperparameters tunning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ur2jxIK0jQ-"
      },
      "source": [
        "##### I) Como se habrá percatado, existe una **inmensa** cantidad de hiperparámetros que uno puede ajustar en las arquitecturas de redes neuronales (learning rate, profundidad de red, n° de neuronas en cada capa, optimizador, cantidad de epochs), también se puede considerar como hiperparámetro si es que incluir o no métodos tales como batch normalization, regularización L1/L2 o dropout en la red neuronal, a la vez habrá qué decidir dónde situar estos métodos, y cómo ajustar sus propios hiperparámetros. Aún más, también es posible experimentar con la manipulación del dataset, p.ej decidir si usar una entrada univariada, multivariada, o un punto intermedio (y si es un punto intermedio qué variables ocupar), también es posible variar el largo de las series de tiempo de entrada.\n",
        "\n",
        "Prepare un código para hacer **Random Search**, dado que los tiempos de entrenamiento serán desproporcionados, **NO ENTRENE** las redes neuronales, pero evalúe en el conjunto de validación como si las hubiese entrenado para escoger los \"\"mejores\"\" parámetros & hiperparámetros de la red neuronal. Mantenga el objetivo de predecir la radiación para las siguientes 24 horas. **Imprima** (print) los valores de los parámetros & hiperparámetros de la red que se esté evaluando, indicando a qué corresponde, y recolecte el error de validación. Realice un Random Search de largo 100, apóyese en el código entregado, **expándalo** desde la búsqueda de 3 hiperparámetros/parámetros hasta una búsqueda de al menos **12** hiperparámetros/parámetros, puede escoger de los ejemplos dados en el primer párrafo de este ítem, o bien escoger por su propia cuenta. El código debe ser funcional, es decir, las _arquitecturas/manipulaciones del dataset_ indicadas por los hiperparámetros/parámetros deben ser efectivamente aplicadas en la construcción de la red neuronal y/o manipulación de datos. Note que en el código dado, un hiperparámetro a ajustar es recurrent dropout. **Indique** cuál fue la mejor arquitectura encontrada según el error de validación.\n",
        "\n",
        "**Preguntas:** \n",
        "\n",
        "i) ¿Qué alternativas al Random Search existen? Nombre al menos una. \n",
        "\n",
        "ii) ¿Qué hace el método recurrent dropout?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPtSbWlNu_jw"
      },
      "source": [
        "val_h=[]\n",
        "for random_search in range(100):\n",
        "  prof_rnn=np.random.randint(1,4)\n",
        "  neu_rnn=[np.random.randint(20,40) for t in range(prof_rnn)]\n",
        "  rec_drop=np.random.uniform(0,0.7)\n",
        "\n",
        "  input_lstm= input_layer.Input(shape=(x_tr.shape[1:]))\n",
        "  for i,neu in enumerate(neu_rnn):\n",
        "    if i==0: rnn=layers.LSTM(units=neu_rnn[0],return_sequences=True,recurrent_dropout=rec_drop)(input_lstm)\n",
        "    else: rnn=layers.LSTM(units=56,return_sequences=(i+1<prof_rnn),recurrent_dropout=rec_drop)(rnn)\n",
        "  dens=layers.Dense(len_out, activation='sigmoid')(rnn)\n",
        "  model=models.Model(inputs=input_lstm, outputs=dens)\n",
        "  model.compile(loss='mse', optimizer='adam')\n",
        "  y_pred=model(x_val)\n",
        "  val_loss=tf.math.reduce_mean(tf.keras.losses.MSE(y_val, y_pred)).numpy()\n",
        "  val_h.append(val_loss)\n",
        "  print(random_search+1,\"° --->  Profundidad rnn: \",prof_rnn, \" | N° neuronas: \", neu_rnn, \" | Recurrent dropout: \", rec_drop)\n",
        "  print(\"Validation error: \",val_loss)\n",
        "  print(\"-\"*120)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RpQzdiS_e03"
      },
      "source": [
        "##### II) Implemente sobre el código de Random Search la funcionalidad de realizar Cross-Validation con 5 Folds para cada red neuronal creada en el proceso de Random Search. Dado que se está trabajando con series de tiempo ocupe la función TimeSeriesSplit. Nuevamente **NO ENTRENE** las redes neuronales. El conjunto de entrenamiento y validación deben ser uno inicialmente para luego ser divididos en cada Fold, el conjunto de test se debiese mantener hasta el final. Calcule el error en el conjunto de validación de cada Fold y use el promedio de los 5 Folds para evaluar el rendimiento de cada red neuronal, encuentre los \"\"mejores\"\" parámetros/hiperparámetros basado en estos promedios.\n",
        "\n",
        "**Preguntas:**\n",
        "\n",
        "i) Visite la página https://scikit-learn.org/stable/modules/cross_validation.html . **Nombre y explique** el método TimeSeriesSplit y al menos otros 3 métodos de cross-validation. ¿Por qué time series split es adecuado para nuestro caso?\n",
        "\n",
        "ii) ¿Cuáles son los fundamentos de Cross Validation en general (como herramienta de selección de hiperparámetros)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3W8RsKaBTNF"
      },
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "data_tr_val, time_tr_val = data_hr[:int(data_hr.shape[0]*0.85)],time_hr[:int(data_hr.shape[0]*0.85)] # DIVIDE IN EACH FOLD\n",
        "data_tst, time_tst = data_hr[int(data_hr.shape[0]*0.85):],time_hr[int(data_hr.shape[0]*0.85):] # KEEP IT UNTIL THE END\n",
        "# se debiese estandarizar en cada Fold ajustando el scaler con la data de entrenamiento que se genere"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVQdG3blVcZs"
      },
      "source": [
        "# 2. RNN sobre texto\n",
        "\n",
        "Hoy en dı́a, una aplicación relevante de las redes neuronales recurrentes es el modelamiento de texto y lenguaje natural. En esta sección abordaremos el problema de procesar sentencias de texto, proporcionadas por GMB (*Groningen Meaning Bank*), para reconocimiento de entidades y tagger. En específico, trabajaremos con el dataset proprocionado a través de __[Kaggle](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)__, que está compuesto por más de un millón de palabras, a fin de realizar predicciones sobre distintas tareas del tipo *many to many* y *many to one*.\n",
        "\n",
        "<img src=\"https://i.stack.imgur.com/b4sus.jpg\" width=\"70%\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr4CUn7PYESQ"
      },
      "source": [
        "## 2.a Carga de datos y preprocesamientos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK7pjS1BZquJ"
      },
      "source": [
        "##### I) Investigue en la documentación del dataset cual es la tarea original para el cual fue propuesto, en particular cuál es la variable que buscamos predecir, a qué se refiere esta misma y por qué es necesario utilizar técnicas avanzadas para resolver esta tarea (¿no bastaría con un diccionario?).\n",
        "\n",
        "Cargue el conjunto de datos. Este conjunto de datos es bastante grande, por lo que como ven en el código propuesto, nos contentaremos con no considerar las lineas corruptas del registro.\n",
        "\n",
        "En esta primera instancia trabajaremos con la tarea de realizar un NER *tag* (**Named Entity Recognition**) sobre cada una de las palabras en las sentencias que se nos presenta en los datos. Esta tarea es del tipo *many to many*, es decir, la entrada es una secuencia y la salida es una secuencia, sin *shift*, por lo que necesitaremos una estructura de red adecuada a ésto. En primer lugar extraiga las columnas que utilizaremos del dataset **¿Por qué es conveniente utilizar *lemma* en vez de la palabra misma *word*?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcORhXvIa5bh"
      },
      "source": [
        "username=\"\"\n",
        "key=\"\"\n",
        "!pip install -q kaggle\n",
        "api_token = {\"username\":username,\"key\":key}\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = str(username)\n",
        "os.environ['KAGGLE_KEY'] = str(key)\n",
        "!kaggle datasets download -d abhinavwalia95/entity-annotated-corpus\n",
        "if not os.path.exists(\"/content/NER\"):\n",
        "    os.makedirs(\"/content/NER\")\n",
        "os.chdir('/content/NER')\n",
        "for file in os.listdir():\n",
        "    if file[-4:]==\".zip\":\n",
        "      zip_ref = zipfile.ZipFile(file, 'r')\n",
        "      zip_ref.extractall()\n",
        "      zip_ref.close()\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YuvuL7vYgur"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df_ner = pd.read_csv(\"ner.csv\", encoding =\"cp1252\", error_bad_lines=False)\n",
        "df_ner.dropna(inplace=True)\n",
        "dataset = df_ner.loc[:,['lemma','tag','word','sentence_idx']]\n",
        "dataset[0:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcfFNDaqYiWn"
      },
      "source": [
        "##### II) Para poder utilizar este conjunto de datos, debemos transformar nuestra tabla de palabras y sentencias, a una tabla donde cada entrada sea una sentencia, ademas codificando los distintos lemmas y tags como valores numéricos. Esto pueden realizarlo con alguna de las utilidades de keras o sklearn, sin embargo en el código siguiente se propone un metodo solo usando python y pandas. Pueden utilizar el método que deseen. Note eso si que independiente la aproximación que utilice debe comenzar desde 1 para la codificación, pues el valor 0 lo reservaremos para representar la ausencia de palabras más adelante.\n",
        "\n",
        "**Explique qué realiza cada linea del código.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bILC6YsLYjJM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "lemma_to_code = {lemma:code+1 for code, lemma in enumerate(dataset.lemma.unique())}\n",
        "tag_to_code = {tag:code+1 for code, tag in enumerate(dataset.tag.unique())}\n",
        "n_lemmas = len(lemma_to_code)\n",
        "\n",
        "dataset['lemma'] = dataset.lemma.apply(lambda x: lemma_to_code[x])\n",
        "dataset['tag'] = dataset.tag.apply(lambda x: tag_to_code[x])\n",
        "\n",
        "dff = dataset.groupby(\"sentence_idx\")[['lemma','tag']].agg(list).applymap(np.asarray)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53KnuIp8Yj4v"
      },
      "source": [
        "## 2.b) Distribuciones.\n",
        "\n",
        "Ahora que ya tenemos las sentencias codificadas y agrupadas, explore el tamaño de estas, en número de lemmas: ¿Son todas las sentencias de igual tamaño? ¿Le hace sentido esto? ¿Las redes que conoce pueden manajar ejemplos de distintos tamaños, y si pueden qué problemas podría traer? ¿Están las clases repartidas de manera equitativa?\n",
        "\n",
        "Estudie la distribución del largo de los textos a procesar. Estudie también la frecuencia con la que aparecen las palabras en todo el dataset. **¿Se observa una ley Zipf?** (https://es.wikipedia.org/wiki/Ley_de_Zipf). Realice un gráfico de la cantidad de datos por clase. Comente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4njiLpNgbWf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgloVFFBYkZQ"
      },
      "source": [
        "## 2.c) Padding y one hot vectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozrWCFFtjAcs"
      },
      "source": [
        "##### I) En esta parte de la tarea, deben lograr que todas las secuencias de lemmas (y los tags correspondientes) queden del mismo largo, es decir realizar padding. El padding debe realizarse con el valor 0, pueden escoger si realizarlo al comienzo de la secuencia o al final, expliquen su elección. Pueden utilizar la función keras.preprocessing.sequence.pad_sequences o escribir sus propios códigos. Elija un valor de maxlen que le parezca adecuado.\n",
        "\n",
        "¿Opinan que es deseable utilizar el valor 0 como codificación de palabras que \"no existen\", o creen que es irrelevante por ejemplo que su valor sea 1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-8M5Ql4YnK7"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW3uZVNnYnoz"
      },
      "source": [
        "##### II) Para poder entregar una clasificación sobre los distintos *tags* es necesario transformarlos a *one hot vectors*, debido a que están codificados en números enteros, esto resultará en un arreglo tridimensional con la cantidad de ejemplos, la cantidad máxima de palabras y la cantidad de posibles *tags*. Luego de esto cree los conjuntos de entrenamiento y de prueba con el código a continuación **¿Cuáles son las dimensiones de entrada y salida de cada conjunto?** Comente\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSZsy681jQgl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "y = np.asarray([to_categorical(i, num_classes=n_tags) for i in Y])\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(X, y, test_size=0.25, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCwsCeU922BV",
        "outputId": "98a4f01f-5e83-4aad-b0bf-d11ba5f909da"
      },
      "source": [
        "x_tr.shape,y_tr.shape,x_val.shape,y_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((26382, 140), (26382, 140, 18), (8795, 140), (8795, 140, 18))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfliTCJkYo8D"
      },
      "source": [
        "## 2.d) RNN many to many\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23LOn_CJoKJh"
      },
      "source": [
        "\n",
        "##### I) Defina una red neuronal recurrente *many to many* con compuertas LSTM para aprender a *tagear* la entidad en el texto, entrene y evalúe su desempeño sobre ambos conjuntos. Esta red debe procesar la secuencia de *lemmas* rellenados (o sin rellenar) y entregar el *tag* a cada uno de los *lemmas*, por lo que la salida de la red no es un vector como anteriormente se ha trabajado, sino que tiene una dimensión extra la cual es debido a que en cada instante de tiempo se necesita entregar un *output*. Como los *lemmas* corresponden a datos esencialmente categóricos, o al menos discretos, es necesario generar una representación vectorial de ellas. La primera capa de la red a construir debe por lo tanto incluir una transformación entrenable desde el espacio de representación original (discreto) a ${\\rm I\\!R}^{d}$ , con $d$ la dimensionalidad del *embedding*. **Comente sobre los cambios que sufre un dato al ingresar a la red y la cantidad de parámetros de la red**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNsItCWwYpsp"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, Dense\n",
        "\n",
        "#¿problemas con el embedding al ejecutar? chequear que el n_lemas, n_tags, y max_len correspondan a los datos modificados con padding\n",
        "m = Sequential()\n",
        "embedding_dim = 32\n",
        "m.add(Embedding(input_dim=n_lemmas, output_dim=embedding_dim, input_length=max_len))\n",
        "m.add(LSTM(units=128,return_sequences=True))\n",
        "m.add(Dense(n_tags, activation='softmax'))\n",
        "m.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])\n",
        "history = m.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=3, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd6E8NBrYqp1"
      },
      "source": [
        "Para evaluar su modelo utilice una métrica adecauda para el desbalance presente entre las clases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvPOBSNVYrII"
      },
      "source": [
        "from sklearn_crfsuite.metrics import flat_classification_report "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbvzZFGrYrnB"
      },
      "source": [
        "##### II) Varı́e la dimensionalidad del embedding inicial y determine si **aumenta o disminuye el error de clasificación**. Comente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "899gszmeFCPU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz961L05YwlU"
      },
      "source": [
        "## 2.e) RNN Bidireccional y masking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGsjPHI7GBFa"
      },
      "source": [
        "##### I) Algunos autores señalan la importante dependencia que existe en texto, no solo con las palabras anteriores, sino que con las que siguen.**Mejore la red definida en 2.d.I) utilizando una red neuronal recurrente Bidireccional**, es decir, con recurrencia en ambas direcciones sobre la secuencia de *lemmas* de entrada. Comente **cuál debiera ser la forma correcta de usar el parámetro *merge_mode*** (concatenar, multiplicar, sumar o promediar) para este caso. Además comente las transformaciones que sufre el patrón de entrada al pasar por las capas. **¿Mejora o empeora el desempeño?** Analice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgiGfK4OYxB9"
      },
      "source": [
        "from keras.layers import Bidirectional\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=n_lemmas, output_dim=embedding_dim, input_length=max_len))\n",
        "layer_lstm = LSTM(units=100,return_sequences=True)\n",
        "model.add(Bidirectional(layer_lstm,merge_mode=choose))\n",
        "model.add(Dense(n_labels, activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])\n",
        "history = model.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=3, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Krl2BVYxox"
      },
      "source": [
        "##### II) Recientemente se ha implementado la capa de *Masking* en las redes recurrentes en *keras*, lo cual podría traer gran ayuda gracias al *padding* que se realiza con el símbolo especial definido. Entrene la red definida en 2.d.I) y compare al utilizar esta funcionalidad de enmascarar el valor 0  en este caso para el default de la capa embedding.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXC5XTgIYyKU"
      },
      "source": [
        "embedding_vector = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=n_lemmas, output_dim=embedding_dim, input_length=max_len,mask_zero=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph_Bc4mIYyiA"
      },
      "source": [
        "## 2.f) Mejora libre\n",
        "\n",
        "En base a lo experimentado, **intente mejorar el desempeño de las redes encontradas**, ya sea utilizando y combinando las distintas variaciones que se hicieron en los distintos ítemes, como bien alguna mejora en el pre-proceso de los datos (largo de secuencia, el tipo de *padding* o alguna otra), agregar mayor profundidad, variar el número de unidades/neuronas, utilizando otra *gate* de recurrencia (GRU o Vanilla/Simple), en https://keras.io/layers/recurrent/,  entre otras. Utilice la red entrenada, **se espera que sea la mejor de esta sección**, y **muestre las predicciones**, el *NER tager*, sobre algún ejemplo de pruebas, comente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve0eznL28D2u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l24Zv52IYzXV"
      },
      "source": [
        "## 2.g) Escribamos palabras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-5OjLw5826S"
      },
      "source": [
        "##### I) Ahora buscaremos otra aplicación a las redes recurrentes, predecir el caracter siguiente. Si logramos entrenar una red que sea buena en esta tarea, podremos escribir texto automáticamente, pues podemos, a partir de una frase, predecir el caracter siguiente, y luego introducir la nueva frase sin el primer caracter en la red nuevamente, e iterando así escribir automáticamente. Si bien las redes recurrentes son adecuadas para esta tarea, no pretendemos entrenar un Shakespeare en esta tarea, sin embargo es interesante investigar qué tan verosimil o no puede lograr ser el texto generado.\n",
        "\n",
        "Para esto, primero crearmos nuestro nuevo dataset. Para esta tarea preferiremos unir todas las frases en un solo gran corpus y luego crear nuevas secuencias semi redundantes. Esto nos evita primero el problema de tener que hacer padding, pues crearemos todas las entradas iguales, pero también nos permite aprovechar mejor el dataset, de cierta forma aumentando el número de datos. El target en este caso será solo el caracter siguiente correspondiente a cada secuencia.\n",
        "\n",
        "En este item debe cargar el dataset. **Explique lo que hace el código entregado**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDLrAoVP8iXA"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "df_w = pd.read_csv(os.path.join(\"ner.csv\"), engine='python', error_bad_lines=False)\n",
        "df_w = df_w.dropna()[['word']]\n",
        "\n",
        "corpus = ' '.join(list(df_w.word.values)).lower()\n",
        "sentence_length = 40\n",
        "steps = 5\n",
        "\n",
        "sentences = []\n",
        "next_char = []\n",
        "for i in range(0,len(corpus) - sentence_length - 1 , steps):\n",
        "  sentences.append(corpus[i:sentence_length+i])\n",
        "  next_char.append(corpus[sentence_length+i])\n",
        "\n",
        "chars_to_code = {char:code for code, char in enumerate(set(corpus))}\n",
        "code_to_chars = {code:char for char,code in chars_to_code.items()}\n",
        "x = np.array([[chars_to_code[char] for char in sentence] for sentence in sentences])\n",
        "y = np.array([chars_to_code[char] for char in next_char])\n",
        "y = to_categorical(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WNuNY2i8oUz"
      },
      "source": [
        "##### II) Entrene ahora una red con estos datos utilizando GRU. El resto de la estructura queda a su elección. Evalúe el desempeño de su red evaluando qué tan bien genera texto, puede utilizar las funciones propuestas como callback para ver como progresa su red. Pruebe a lo menos 2 estructuras distintas.\n",
        "\n",
        "Una vez esté satisfecho de su red, hágala escribir algunos textos a partir de textos semilla elegidos por usted. Describa sus observaciones. ¿Qué cree ocurriría si entrenamos la red con otro dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEQzpcMV85a_"
      },
      "source": [
        "from keras.layers import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import GRU\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "\n",
        "def predict_char(model, sentence):\n",
        "    x = [chars_to_code[char] for char in sentence]\n",
        "    x = pad_sequences([x], maxlen=sentence_length, padding='pre', value=0)\n",
        "    probas = model.predict(x)[0]\n",
        "    next_index = np.random.choice(len(chars_to_code), p=probas)\n",
        "    return code_to_chars[next_index]\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "    print(f'\\n Generating random text for epoch: {epoch}')\n",
        "    start_index = random.randint(0,x.shape[0]-1)\n",
        "    sentence = ''.join([code_to_chars[code] for code in x[start_index]])\n",
        "    print('\\n Generating with seed: ' + sentence)\n",
        "    sys.stdout.write(sentence)\n",
        "    for i in range(400):\n",
        "        next_char = predict_char(character, sentence)\n",
        "        sentence = sentence[1:] + next_char #for next character\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    return\n",
        "\n",
        "print_text_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqLb5kym9JRM"
      },
      "source": [
        "character = Sequential()\n",
        "embedding_dim = 100\n",
        "character.add(Embedding(input_dim=y.shape[1], output_dim=embedding_dim, input_length=x.shape[1]))\n",
        "character.add(GRU(128,return_sequences=False))\n",
        "character.add(Dense(y.shape[1],activation='softmax'))\n",
        "optimizer = RMSprop(lr = 0.01)\n",
        "character.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=[\"acc\"])\n",
        "character.fit(x,y, epochs=1, batch_size = 512,callbacks=[print_text_callback])\n",
        "character.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7d4jCpD_qf5"
      },
      "source": [
        "# 3. Autoencoders en Fashion MNIST\n",
        "\n",
        "Si bien las redes neuronales han tenido desempeños sorprendentes en muchas áreas donde antes solo un ser humano podía alcanzar buenos desempeños, uno de sus desventajas suele serla alta dimensionalidad de los espacios de hipótesis. En la práctica, esto implica que para aprender una tarea predictiva con alguna capacidad de generalización, se requieren grandes bases de datos etiquetadas. Esto implica un problema, considerando que al momento de buscar la base de datos, no se tiene a priori una manera de automatizar esta etiquetación. Esta necesidad de gran cantidad de trabajo de clasificación realizado por humanos, ha engendrado soluciones ingeniosas, como la aproximación de _Facebook_ hace algunos años de pedirle a los mismos usuarios que etiquetaran a las personas en sus fotos, o la solución de _Amazon_, _Mechanical Turk_, donde cualquier usuario puede realizar tareas repetitivas de clasificación a cambio de dinero real, o por otro lado cualquier persona puede comprar la etiquetación de una base de datos la cual realizan varias personas en cualquier parte del mundo. \n",
        "\n",
        "Otra aproximación, quizás aún más ingeniosa, para solucionar el problema de las etiquetas, es utilizar las bases de datos sin preocuparse de sus etiquetas. Esta aproximación de aprendizaje no supervisado tiene su representante en redes neuronales en los _Autoencoders_, redes que utilizan el mismo input como target y buscan representaciones de menor dimensionalidad al interior de la red. Estas redes han permitido el uso de cantidades masivas de datos para aprender de ellos sin necesidad de tener etiquetas. Durante esta pregunta veremos algunos de los aspectos y posibilidades básicas que nos presentan los _autoencoders_, utilizando una base de datos de imágenes de articulos de vestimenta, el Fashion MNIST. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbD2z6RtAKh8"
      },
      "source": [
        "## 3.a) Carga de datos y visualizaciones\n",
        "Cargue los datos. Puede user las funciones de `keras.datasets` como muestra el código o descargarlo manualmente. \n",
        "\n",
        "Luego, visualice algunas imagenes de cada una de las catégorias junto con sus nombres (investigue un poco para encontrar la codificación de `y`). Note que las imagenes deben representarse en blanco y negro, puede usar `cmap='Greys'`. ¿Qué pares de categorías cree podrían ocasionar problemas al momento de clasificación? ¿Qué tan bien cree que se desempeñaría un humano en esta tarea?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99fkahmFBko7"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_tr,y_tr),(x_val,y_val) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTUuE2uOBo64"
      },
      "source": [
        "## 3.b Posibilidades de preprocesamiento y pequeños análisis. \n",
        "\n",
        "Las primeras redes que entrenaremos utilizarán arquitecturas _fully connected_, por lo cual también es necesario transformar nuestras imagenes 2-dimensionales a vectores, como muestra el ejemplo de código.\n",
        "\n",
        "**Preguntas:**\n",
        "\n",
        "i) ¿Cuáles son los rangos de valores de `x`? ¿Por qué?\n",
        "\n",
        "ii) ¿Las distintas clases de ejemplos están balanceadas?\n",
        "\n",
        "iii) ¿Considera necesario realizar un preprocesamiento? Escale los valores de `x` al intervalo $[0,1]$, y guarde el conjunto de datos original de igual manera que el escalado. ¿Se pierde información al realizar este preprocesamiento? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YCenZW8Bz7D"
      },
      "source": [
        "x_tr_vector = x_tr.reshape(-1,28*28)\n",
        "x_val_vector = x_val.reshape(-1,28*28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCXwbgSCB_WS"
      },
      "source": [
        "## 3.c) Primer Autoencoder\n",
        "\n",
        "Entrenaremos un primer autoencoder de una capa oculta, usando arquitectura densa. Para esto, utilize como guía los códigos presentados abajo. \n",
        "\n",
        "Utilice en primera instancia su conjunto de datos escalados. Considerando el intervalo de los datos escalados.\n",
        "\n",
        "Entrene esta primera red utilizando pérdida _binary cross entropy_. Compare luego las imagenes originales con las imagenes reconstruidas, como muestra el código. **Grafique** como varia la pérdida a lo largo del entrenamiento y visualice algunas imagenes reconstruidas. \n",
        "\n",
        "**Preguntas:**\n",
        "\n",
        "i) ¿Qué función de activación correspondería a la capa de salida de la red? ¿Debería afectar la elección de la función de activación de la capa oculta? \n",
        "\n",
        "ii) ¿Qué le parece el desempeño de la red, logra aprender la tarea en su opinion?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Oi_avWB91h"
      },
      "source": [
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras.models import Sequential\n",
        "\n",
        "autoencoder = Sequential()\n",
        "\n",
        "autoencoder.add(Dense(32,activation='')) # encoder\n",
        " \n",
        "autoencoder.add(Dense(28*28,activation= '')) #decoder\n",
        "\n",
        "autoencoder.compile(optimizer=SGD(lr=0.002),loss='binary_crossentropy')\n",
        "autoencoder.fit(x_train_vector_scaled,x_train_vector_scaled,epochs=50,validation_data=(x_val_vector_scaled,x_val_vector_scaled))\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"]=[16,4]\n",
        "ix = 1\n",
        "for u in range(5):\n",
        "  for v in range(2):\n",
        "    ax = plt.subplot(2,5,ix)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    plt.imshow(autoencoder.predict(x_val_vector_scaled)[ix].reshape(28,28))\n",
        "    ix += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ImTXSAMCDQw"
      },
      "source": [
        "## 3.d) Dimensionalidad\n",
        "\n",
        "Una forma de interpretar lo que realiza el autoencoder, es considerar que si el autoencoder hace bien su tarea, la información necesaria para reconstruir la imagen original se encuentra en la capa oculta, la cual tiene menor dimensionalidad que la imagen original. Uno puede considerar por lo tanto que la capa de _encoding_ esta comprimiendo la información contenida en la imagen, mientras la capa de _decoding_ hace el proceso contrario, descomprimiendola a su estado original lo mejor posible. \n",
        "\n",
        "Explore como cambia el desempeño de la red en cuestión frente a cambios en la dimensión de la capa oculta. Pruebe a lo menos 5 niveles de compresión distintos, incluyendo uno donde la capa oculta tenga $50\\%$ de ratio de compresión y otro donde la capa oculta tenga tan solo 2 neuronas. ¿Qué observa?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY4JQWakB-Z0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE2yrtsvB-h5"
      },
      "source": [
        "## 3.e) Deep autoencoder\n",
        "\n",
        "Pruebe ahora con una arquitectura ligeramente más profunda. Para esto utilice a lo menos 3 capas de encoding, es decir, 3 capas que progresivamente reduzcan la dimensionalidad de la representación hasta una dimensión objetivo inicialmente igual a la mejor obtenida en la pregunta anterior. Utilice igualmente a lo menos 2 capas de decoding, que se encarguen de aumentar la dimensionaliad de la representación hasta alcanzar la dimensión de la imagen original. Note que las primeras capas no necesariamente deben tener menor dimensionalidad que la imagen, la dimensión relevante es aquella de la última capa de encoding. \n",
        "\n",
        "Una vez esté satisfecho con su arquitectura profunda, varíe la dimensión objetivo de la última capa de encoding, realizando una exploración similar a la pregunta anterior. Utilice gráficos y muestre algunas imágenes reconstruidas para complementar sus comentarios. \n",
        "\n",
        "**Preguntas:**\n",
        "\n",
        "i) ¿Cómo aumenta el número de parámetros entrenables? ¿Aumenta el tamaño de la representación \"comprimida\"?\n",
        "\n",
        "ii) ¿Puede obtener una representación de menor dimensionalidad que la encontrada en el item anterior sin perder calidad en las imagenes obtenidas?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZME3f9IOB-ra"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xQG4SM3Dxgr"
      },
      "source": [
        "## 3.f) Convolutional Autoencoder\n",
        "\n",
        "Como hemos hasta ahora  utilizado una arquitectura fully connected, nuestra red no toma en cuenta la infomación local contenida en la proximidad de un pixel en la imagen. Como vimos en la tarea anterior, esta información podría ser crucial al momento de procesar imágenes. \n",
        "\n",
        "En este item deberá implementar un autoencoder convolucional. La sección de encoding de la red se creará de igual manera que las redes convolucionales creadas en la tarea 1. Puede utilizar capas de Max Pooling o Strides mayores a 1 para reducir la dimensionalidad en esta etapa. Tenga en mente como varía la dimensión de la imagen a lo largo del proceso. \n",
        "\n",
        "Para luego recuperar la dimensionalidad de la imagen original debemos utilizar una capa llamada usualmente como \"Deconvolution Layer\". Esta capa realiza el proceso inverso que aquel realizado por una capa convolucional, por lo cual utilizando por ejemplo `stride=2` puede duplicar la dimensionalidad de su input. \n",
        "\n",
        "Puede realizar la profundidad que desee, pero tome en cuenta que una mayor profundidad de la sección convolucional permite a la red reducir dimensionalidad más lentamente, sin \"forzar\" la compresión de las características. Note que para reconstruir la dimensión original puede usar `output_padding` para corregir problemas de paridad, entre otros. \n",
        "\n",
        "Puede igualmente optar por usar algun número de capas densas en el cuello de botella del autoencoder, usando al comienzo de esta una capa `Flatten` y al final de ella una capa `Reshape` (`keras.layers.Reshape(target_shape)`) para recuperar la bidimensionalidad.\n",
        "\n",
        "¡No olvide que para entrenar esta red debe usar la versión bidimensional de los datos escalados!\n",
        "\n",
        "Visualice que tan bien se comporta la convolución, en terminos de la función de pérdida y visualizando las imagenes reconstruidas. Compárese con la red densa. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE2Zu8LaDyVg"
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras.models import Sequential\n",
        "conv=Sequential()\n",
        "#E\n",
        "conv.add(Conv2D(filters=?, kernel_size=(?,?), padding='same', activation='relu',input_shape=?))\n",
        "conv.add(MaxPooling2D(pool_size=(?,?)))\n",
        "conv.add(Conv2D(filters=?, kernel_size=(?,?), padding='same', activation='relu',input_shape=?))\n",
        "conv.add(MaxPooling2D(pool_size=(?,?)))\n",
        "\n",
        "#D\n",
        "conv.add(Conv2D(filters=?, kernel_size=(?,?), padding='same', activation='relu'))\n",
        "conv.add(UpSampling2D((?, ?)))\n",
        "conv.add(Conv2D(filters=?, kernel_size=(?,?), padding='same', activation='relu'))\n",
        "conv.add(UpSampling2D((?, ?)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4gXQvRxEZYf"
      },
      "source": [
        "## 3.g) Denoising Autoencoder \n",
        "\n",
        "Otra utilidad que se le ha dado a los autoencoders es la posibilidad de utilizarlos para separar ruido de información. Para entrenar tal tipo de modelo, la idea es simple: utilizar como datos de entrada imagenes a las cuales se les ha agregado artificialmente ruido y como objetivo la imagen original sin ruido. \n",
        "\n",
        "Entrene alguna arquitectura de autoencoder que le parezca apropiada para la tarea utilizando algún tipo de ruido aleatorio. Puede utilizar cambios en valores de pixeles aleatoriamente, o por ejemplo \"promediar\" ponderadamente la imagen original con alguna otra imagen del dataset ligeramente modificada. Puede utilizar las librerías `random` de `numpy`. Sea creativo, puede crear el ruido que desee. Idealmente, considerando la naturaleza del problema que se buscaría modelar (eliminar ruido real de mediciones), la naturaleza del ruido agregado debe ser estocástica y no puede \"repetirse\" el mismo patrón de ruido a lo largo de todo el entrenamiendo, es decir, si agregó un ruido estocástico a cada imágen del conjunto, este proceso debe iterarse igualmente luego de cada época de entrenamiento, para evitar que la red aprenda un patrón especifico de ruido, si no aprenda realmente a diferenciar ruido sin información de la información suyaciente a la imagen. \n",
        "\n",
        "Una vez esté satisfecho con la red, muestre ejemplos de la imagen con ruido, la imagen original y la imagen reconstruida. Pruebe igualmente entregarle a la red nuevas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMpFu3pqEY0I"
      },
      "source": [
        "autoencoder.fit(x_tr_noise,x_tr,epochs=50,validation_data=(x_val_noise,x_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5tB9YAVFD5Q"
      },
      "source": [
        "## 3.h) Generación de imágenes\n",
        "\n",
        "Otra utilización posible que podría darse, quizás, a los autoencoders, es utilizar los decoders para generar nuevas imágenes. La idea de esto sería considerar que la habilidad que tiene la sección decoder de generar una imágen a partir de una representación de menor dimensionalidad puede aprovecharse. \n",
        "\n",
        "Para esto, extraiga la sección de encoding y la sección de decoding de alguno de los autoencoders entrenados que prefiera. Obtenga los valores de la representación interna de las imágenes usando el encoder para calcularlos. Luego agréguele a esta representación algún ruido de su preferencia, y calcule la imágen resultante utilizando el decoder. ¿Qué observa? Muestre las imágenes obtenidas junto con otras imágenes de la misma categoría que la imágen que utilizó originalmente. Pruebe con distintos valores de ruido.\n",
        "\n",
        "Pruebe también, por ejemplo, calcular la imágen obtenida al promediar las representaciones comprimidas de varias (o todas) las imágenes de una clase. Utilice el encoder para generar imágenes a partir de otros valores que se les ocurran y especule sobre el por qué la imágen obtenida se asimila o no a las imágenes del dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqDKVkdaOLYn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj6dOieOGWMf"
      },
      "source": [
        "# 4. GAN para MNIST \n",
        "\n",
        "Probablemente uno de los desarrollos recientes del area de las redes neuronales más interesantes son las GAN, o _Generative Adversarial Networks_. Estas han deslumbrado al mundo los últimos años generando resultados inesperados, como los llamados _deep fakes_ (https://www.youtube.com/watch?v=25GjijODWoI&ab_channel=Borked), caras no no existentes generadas artificialmente, entre muchas otras aplicaciones de las cuales las más creativas y divertidas son ampliamente divulgadas. Estos resultados nos dan cuenta que estas redes cuando son implementadas correctamente tienen la habilidad de realizar tareas muy especificas logrando desempeños que en logran incluso en algunos casos engañar a observadores humanos.\n",
        "\n",
        "<img src=\"https://jrmerwin.github.io/deeplearning4j-docs/img/GANs.png\" background=\"white\">\n",
        "\n",
        "Dependiendo del problema que uno quiera resolver las GAN no requieren datos etiquetados. Esto lo logran gracias a su estructura adversarial, es decir, donde simultaneamente se entrenan dos redes, una especializada en generar datos y otra red especializada en discriminar datos verdaderos de datos falsos, en su configuración más simple al menos. En esta parte de la tarea deberan generar una red de este tipo que genere dígitos que aparenten ser hechos a mano. Para esto utilizaremos las imágenes de entrenamiento del dataset MNIST que ya conocen.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbKL3rzYJO9f"
      },
      "source": [
        "## 4.a) Primera GAN\n",
        "\n",
        "El código siguiente contiene todos los ingredientes para entrenar una red GAN, se encuentra áltamente comentada para su entendimiento. Usted deberá incluir funcionalidades al código, no incluya aún ninguna mejora a la arquitectura de las redes, esto se verá en el item 4.c). Las funcionalidades que debe añadir son las siguientes: \n",
        "\n",
        "i) Imprimir alguna medida del desempeño tanto del generador como del disciminador (por ejemplo f1-score, precision and recall o accuracy) a lo largo del entrenamiento. ¿Variaciones en esta medidas representan mejoras en nuestra red?\n",
        "\n",
        "ii) Añadir gráficos de las entradas de data real con un título que indique la clasificación que está dando el discriminador a cada una de estas imágenes (Falsa|Real). El código entregado grafica tan solo las imágenes que genera el generador y la etiqueta que le está dando el discriminador (Falsa|Real).\n",
        "\n",
        "**Recuero usar entorno de ejecución en GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWnnyGEaOVJO"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(x,_),(_,_) = mnist.load_data()\n",
        "x = x.reshape((-1,28,28,1)).astype(np.float32)\n",
        "x = x/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn8dGeHNJSoB"
      },
      "source": [
        "from keras.layers import Flatten\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import UpSampling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRE2oqT_MOd-"
      },
      "source": [
        "discriminator_net = Sequential() # Discriminador\n",
        "discriminator_net.add(Conv2D(3, 3, strides=2, input_shape=x.shape[1:], padding='same')) #Recibe como entrada tanto imágenes generadas por el generador como imágenes del dataset\n",
        "                                                                                        # de entrenamiento. Recuerden que los hiperparámetros de la red son intencionalmentes\n",
        "                                                                                        # no idóneos.\n",
        "discriminator_net.add(Flatten())    #Flatten para pasar a una neurona de capa densa\n",
        "discriminator_net.add(Dense(1,activation='sigmoid',use_bias=False))   # La salida estará entre 0 y 1.\n",
        "\n",
        "generator_net = Sequential() \n",
        "noise_size=50\n",
        "generator_net.add(Dense(7*7*10, input_shape=[noise_size]))   # el generador recibirá como entrada ruido, el tamaño es arbitrario.\n",
        "generator_net.add(Reshape((7,7,10)))    # Transforma la salida de la red densa unidimensional a tridimensional, las primeras dos pueden ser interpretadas como tamaño imágen\n",
        "                                          # y la tercera como n° canales\n",
        "generator_net.add(UpSampling2D())   # upsampling2d nos permite pasar de imágenes 7,7,10 a 14,14,10\n",
        "generator_net.add(Conv2D(32, 5, padding='same')) # podemos añadir convolucionales dada la entrada a esta capa, la primera dimensión transformará la data\n",
        "                                                      # desde 14,14,10 a 14,14,32\n",
        "generator_net.add(UpSampling2D()) # pasamos de 14,14,32 a 28,28,32\n",
        "generator_net.add(Conv2D(1, 6, padding='same',activation='sigmoid')) # hagamos lo que hagamos hay que asegurarse de obtener una salida 28,28,1 como es el caso en esta arquitectura\n",
        "\n",
        "discriminator = Sequential() # Compilamos el discriminador, sin embargo debemos crear la red auxiliar a discriminator_net, para poder entrenarla independiente del generator\n",
        "discriminator.add(discriminator_net)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer = RMSprop(lr=1e-2), metrics=['acc']) # compilamos con binary cross entropy\n",
        "\n",
        "gan = Sequential() # GAN contendrá al generador y al discriminador en serie\n",
        "gan.add(generator_net) # añadimos inicialmente el generador\n",
        "for layer in discriminator_net.layers: # y luego el discriminador \n",
        "    layer.trainable = False # nos preocupamos de dejar esta capa no entrenable por el compilador, puesto este discrimiador se entrenará con la data real para identificar real|fake\n",
        "gan.add(discriminator_net) # añadimos las capas ya modificadas para no ser entrenadas\n",
        "gan.compile(loss='binary_crossentropy', optimizer = RMSprop(lr=1e-2), metrics=['acc']) #compilamos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwhXDrPDKLiD"
      },
      "source": [
        "batch_size=128 # tamaño del batch\n",
        "plt.rcParams[\"figure.figsize\"]=[20,4]\n",
        "for epoch in range(20): #epochs a realizar\n",
        "    for batch in range(int(x.shape[0]/batch_size)): #n° de batchs a realizar según el tamaño batch_size\n",
        "      # train discriminator\n",
        "      ix = np.random.randint(0, x.shape[0], batch_size) # generamos índices para samplear imágenes del dataset, sampleamos batch_size=128 datos\n",
        "      x_real = x[ix]  # seleccionamos las imágenes\n",
        "      noise = np.random.rand(noise_size * batch_size) # generamos ruido, según el tamaño de la entrada de nuestro generador\n",
        "      noise = noise.reshape((batch_size, noise_size)) # reshape (batch_size,noise_size) sized noise\n",
        "      x_false = generator_net.predict(noise) # generamos imágenes fake según el ruido\n",
        "      y_real= np.ones((batch_size, 1)) # etiquetas real\n",
        "      y_false = np.zeros((batch_size, 1)) #etiquetas fake\n",
        "      x_train = np.concatenate((x_real, x_false)) # x_train-> imágenes reales + imágenes fakes\n",
        "      y_train = np.concatenate((y_real, y_false)) # y_train-> etiquetas reales + etiquetas fakes\n",
        "      response_dis = discriminator.train_on_batch(x_train, y_train) #train discriminator\n",
        "      # train generator\n",
        "      noise2 = np.random.rand(noise_size * batch_size) # generamos ruido nuevamente según el tamaño de la entrada de nuestro generador\n",
        "      noise2 = noise2.reshape((batch_size, noise_size)) # reshape (batch_size,noise_size) sized noise\n",
        "      response_gen = gan.train_on_batch(noise2 , np.ones((batch_size, 1))) # output is 1, as generator needs to convine discriminator (trainable false) w/ fake images from noise\n",
        "      if batch%(200)==0: # cada cuánto plotear es ajustable\n",
        "          # plot random generated images, discriminator image inputs and outputs, and losses and accuracies\n",
        "          plt.suptitle(\"Generator generated images\", y=1.2,size=22)\n",
        "          for i in range(10):\n",
        "            plt.subplot(2, 5, 1 + i)\n",
        "            plt.title(\"Discriminator says:\\n\"+ [\"Fake\",\"Real\"][int(np.round(discriminator.predict(x_false[i:i+1, :, :, :])[0,0],0))])\n",
        "            plt.axis('off')\n",
        "            plt.imshow(x_false[i, :, :, 0], cmap='gray_r')\n",
        "          plt.tight_layout()\n",
        "          plt.show()\n",
        "          print(\"-\"*120)\n",
        "          \n",
        "          # plt.suptitle(\"Real images\", y=1.2,size=22)\n",
        "          # for i in range(10):\n",
        "          #   plt.subplot(2, 5, 1 + i)\n",
        "            # plt.title(\"Discriminator says:\\n\"+ [\"Fake\",\"Real\"])\n",
        "            # plt.imshow(,cmap='gray_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1c--d_QYWro"
      },
      "source": [
        "## 4.b) Mejora de GAN.\n",
        "\n",
        "Utilizando el código modificado en el punto 4.b y lo aprendido durante el ramo. Modifique la arquitectura de la red neuronal a modo de obtener imágenes fake sean convincentes. Adicionalmente responda/comente según lo que observa durante el entrenamiento de la red neuronal.\n",
        "\n",
        "* Explicar el comportamiento de la evolución de ambos desempeños y por qué no necesariamente esos valores representan que la red alcance su cometido\n",
        "* Describir, teórica o práctiamente, que ocurriría si la red generadora no pudiera por algún motivo (divergencia en entrenamiento, excesivo _underfitting_, etc) generar imágenes razonables.\n",
        "* Describir la contraparte de lo que ocurriría si la red discriminadora no pudiera aprender a diferenciar imagenes reales de ruido aleatorio.\n",
        "\n",
        "\n",
        "Una vez teniendo una red entrenada a completitud, muestre varias imagenes generadas. ¿Se logra obtener imágenes convincentes?. Igualmente con la red entrenada, guarde el ruido aleatorio que origina dos números reconocibles distintos. ¿Qué ocurre si vemos las imagenes generadas por el generador al entregarle el vector del promedio entre los dos puntos?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruiWtrOdb6wF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoKZw42-Goxm"
      },
      "source": [
        "## Pregunta **Bonus**\n",
        "\n",
        "* Proponga o investigue como realizaría las siguientes tareas:\n",
        "    * A partir de una base de datos de imagenes RGB, entrenar una GAN que genere imágenes a color convincentes a partir de imagenes en blanco y negro, es decir que deduzca el color a partir de imagenes en blanco y negro.\n",
        "    * A partir de una base de datos de cuadros de paisajes reales y las fotos correspondientes al cuadro, entrenar una GAN que permita transformar fotografias a cuadros y viceversa (puede utilizar más de 2 redes)\n",
        "    * Proponga un problema que les parezca interesante y una estructura de GAN que le permitiría resolverlo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFeVyF3VGFP2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}